#!/bin/bash
#SBATCH -t 165:00:00
#SBATCH --ntasks=2
#SBATCH --nodes=1
#SBATCH --job-name=simple
#SBATCH --account=all6000users
#SBATCH --partition=all6000
#SBATCH --gres=gpu:2
#SBATCH --mem=128G
#SBATCH --cpus-per-task=16
#SBATCH --output=./outputs/slurm_output_%A.out
#SBATCH --exclude=ivi-cn005,ivi-cn009,ivi-cn010,ivi-cn011,ivi-cn012,ivi-cn001
#SBATCH --export=ALL,WANDB_API_KEY=dfcd2574507b9ebe69ca13ab6f6925d864e82ee0

echo "TX Adaptivity Experiments"
echo | date
echo "Node name: $(hostname)"
echo -n memory=; ulimit -m
echo -n nproc=; nproc

# Initialize Conda and activate environment
eval "$(conda shell.bash hook)"
conda activate prune_llm

# Distributed Training Setup (if using DDP)
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=$((15000 + RANDOM % 5000))  # Use random port
export WORLD_SIZE=$SLURM_NTASKS
export RANK=$SLURM_PROCID
export LOCAL_RANK=$SLURM_LOCALID
export CUDA_LAUNCH_BLOCKING=1
export TORCH_USE_CUDA_DSA=1

nvidia-smi

# srun python compare_tensor.py

# srun python cnn_iterative_adaptivity.py \
#     --exp_name Resnet50_Iter_Adaptivity_SGD \
#     --model_name resnet50 \
#     --weights ResNet50_Weights.IMAGENET1K_V2 \
#     --pruning_type l1 \
#     --pruning_ratio 0.5 \
#     --iterative --pruning_steps 5 \
#     --taylor_batchs 10 \
#     --data_path /nvmestore/koelma/pytorch_work/ilsvrc2012/ \
#     --train_batch_size 128 \
#     --val_batch_size 128 \
#     --save_as saves/state_dicts/ \
#     --test_accuracy \
#     --rebuild \
#     --epochs 50 --core_epochs 50 --lr-warmup-epochs 5 --lr 0.05 --opt sgd \
#     --mixup-alpha 0.8 --core_weight_decay 1e-4 --stochastic_depth \
#     --clip-grad-norm 1 --amp --ra-sampler \
#     --distributed --model-ema --log_wandb \


srun python ViT_iterative_adaptivity.py \
    --exp_name deit_Iter_Adaptivity_cifar100 \
    --dataset_name cifar100 \
    --model_name facebook/deit-base-patch16-224 \
    --pruning_type l1 \
    --pruning_ratio 0.5 \
    --iterative --pruning_steps 5 \
    --taylor_batchs 10 \
    --data_path /nvmestore/koelma/pytorch_work/ilsvrc2012/ \
    --train_batch_size 128 \
    --val_batch_size 128 \
    --save_as saves/state_dicts/ \
    --test_accuracy \
    --rebuild \
    --epochs 50 --core_epochs 50 --lr-warmup-epochs 5 --lr 0.05 \
    --mixup-alpha 0.8 --core_weight_decay 0.05 --stochastic_depth \
    --clip-grad-norm 1 --amp --ra-sampler \
    --distributed --model-ema --log_wandb \


# srun python ViT_iterative_adaptivity.py \
#     --exp_name ViT_Iter_Adaptivity_test_multistep \
#     --model_name google/vit-base-patch16-224 \
#     --pruning_type l1 \
#     --pruning_ratio 0.8 \
#     --iterative --pruning_steps 10 \
#     --taylor_batchs 10 \
#     --data_path /nvmestore/koelma/pytorch_work/ilsvrc2012/ \
#     --train_batch_size 128 \
#     --val_batch_size 128 \
#     --save_as saves/state_dicts/ \
#     --test_accuracy \
#     --rebuild \
#     --epochs 20 --core_epochs 20 --lr-warmup-epochs 0 \
#     --mixup-alpha 0.8 --core_weight_decay 0.05 --stochastic_depth \
#     --clip-grad-norm 1 --amp --ra-sampler \
#     --debug \


echo "Job Complete"
echo | date